{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd6foibzwQ4+zPlHvENxca",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolenki7/ETL_Egatur_INE_Esp/blob/main/Egatur_Power_BI_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCWzFNmsDRol",
        "outputId": "6fd28bff-81d4-49cd-87f2-9e57f565ed18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.11\n",
            "Librerías instaladas e importadas.\n",
            "Configuración de DB: postgres@tu_ip_publica_o_local:5432/egatur_db\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------\n",
        "# 1. INSTALACIÓN E IMPORTACIÓN DE LIBRERÍAS\n",
        "# ----------------------------------------------------\n",
        "# psycopg2 es la librería para conectarse a PostgreSQL\n",
        "!pip install psycopg2-binary pandas\n",
        "\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import io\n",
        "import os\n",
        "from google.colab import files # Para manejar la carga de archivos\n",
        "print(\"Librerías instaladas e importadas.\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. CONFIGURACIÓN DE POSTGRESQL (¡AJUSTAR CREDENCIALES!)\n",
        "# ----------------------------------------------------\n",
        "DB_HOST = 'tu_ip_publica_o_local'  # Asegúrate de que tu IP local o 'localhost' funcione\n",
        "DB_NAME = 'egatur_db'\n",
        "DB_USER = 'postgres'\n",
        "DB_PASSWORD = '5432' # Tu contraseña\n",
        "DB_PORT = 5432\n",
        "TABLE_DESTINO = 'egatur_datos_maestros'\n",
        "\n",
        "# Lista explícita de los 5 archivos a procesar (excluyendo el agregado)\n",
        "FILES_TO_PROCESS = [\n",
        "    '13938.csv',    # Partidas de gasto\n",
        "    '10838.csv',    # País de residencia\n",
        "    '10839.csv',    # Comunidades autónomas (Destino)\n",
        "    '10828.csv',    # Vía de acceso\n",
        "    '23995.csv'     # Motivo del viaje\n",
        "]\n",
        "\n",
        "print(f\"Configuración de DB: {DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 3. EXTRACCIÓN DIRECTA DE ARCHIVOS ADJUNTOS\n",
        "# ----------------------------------------------------\n",
        "# Nota: En Google Colab, para acceder a archivos adjuntos en el chat,\n",
        "# se necesita usar un método específico.\n",
        "# Si el entorno persiste, el método más robusto es cargar el contenido en la memoria\n",
        "# de Colab para que la función clean_and_unify_egatur() pueda acceder a ellos.\n",
        "\n",
        "# Mapeo de los nombres de archivo a sus identificadores de contenido (ContentFetchId)\n",
        "# Estos son los 5 archivos clave, excluyendo el agregado.\n",
        "FILE_CONTENT_MAP = {\n",
        "    '13938.csv': 'uploaded:13938.csv',\n",
        "    '10838.csv': 'uploaded:10838.csv',\n",
        "    '10839.csv': 'uploaded:10839.csv',\n",
        "    '10828.csv': 'uploaded:10828.csv',\n",
        "    '23995.csv': 'uploaded:23995.csv'\n",
        "}\n",
        "\n",
        "# La lista de archivos que el script procesará (la misma que en la Celda 1)\n",
        "FILES_TO_PROCESS = list(FILE_CONTENT_MAP.keys())\n",
        "\n",
        "# Crear un diccionario \"uploaded\" para simular la subida (E)\n",
        "# Usaremos una función para obtener el contenido binario de los archivos adjuntos.\n",
        "\n",
        "# [Aquí iría una función auxiliar para obtener el contenido, la omitimos por brevedad]\n",
        "# Para simular esto en un entorno Colab simple, utilizaremos una función que\n",
        "# asume que el contenido ya está disponible en el entorno de Python local/Colab.\n",
        "\n",
        "# Si estás en un entorno local (no en la nube de Colab):\n",
        "# Asegúrate de que los archivos están en la carpeta 'data/' y usa pd.read_csv('data/13938.csv', ...)\n",
        "\n",
        "# Si estás en Google Colab: Debes subir los archivos o cargarlos desde Drive.\n",
        "# Si la carga falló, volvamos a la forma más segura para Colab.\n",
        "try:\n",
        "    # Intentamos la carga interactiva (es la más estable en Colab)\n",
        "    print(\"Para asegurar el éxito, subiremos los archivos de nuevo. Solo toma unos segundos.\")\n",
        "    uploaded_files = files.upload()\n",
        "    uploaded = uploaded_files # Usaremos este nombre en la siguiente celda\n",
        "except Exception as e:\n",
        "    print(f\"Error al intentar la carga: {e}\")\n",
        "    print(\"Si está en un notebook persistente, la carga puede no ser necesaria.\")\n",
        "    # Si la carga falla, el script no puede proceder, ya que la memoria de Colab es volátil.\n",
        "    # Por favor, sube los archivos de nuevo si el entorno se reinició.\n",
        "    raise\n",
        "\n",
        "# Verificación de archivos clave para la celda 3\n",
        "if not all(f in uploaded for f in FILES_TO_PROCESS):\n",
        "    missing = [f for f in FILES_TO_PROCESS if f not in uploaded]\n",
        "    print(f\"\\n❌ ERROR CRÍTICO: No se encontraron los siguientes archivos para la ETL: {missing}. Por favor, vuelve a cargarlos.\")\n",
        "else:\n",
        "    print(\"\\n✅ Archivos cargados y listos para la transformación.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "Xf9TdPSUFROX",
        "outputId": "c7ed2cbb-7afd-402a-b5c4-2685aa522d96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para asegurar el éxito, subiremos los archivos de nuevo. Solo toma unos segundos.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-03b594d0-bdc5-4fb9-ad86-ddb2141a9357\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-03b594d0-bdc5-4fb9-ad86-ddb2141a9357\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 23995.csv to 23995.csv\n",
            "Saving 13938.csv to 13938.csv\n",
            "Saving 10828.csv to 10828.csv\n",
            "Saving 10838.csv to 10838.csv\n",
            "Saving 10839.csv to 10839.csv\n",
            "\n",
            "✅ Archivos cargados y listos para la transformación.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 4. FUNCIÓN DE TRANSFORMACIÓN, LIMPIEZA Y UNIFICACIÓN (¡VERSIÓN FINAL Y COMPLETA!)\n",
        "# ----------------------------------------------------\n",
        "def clean_and_unify_egatur():\n",
        "    \"\"\"Limpia los CSV del INE, los normaliza y los consolida.\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    # Columnas que contienen la métrica o la estructura temporal (a EXCLUIR para encontrar la dimensión)\n",
        "    EXCLUSION_COLS = ['Tipo de dato', 'Gastos y duración media de los viajes', 'Periodo', 'Total',\n",
        "                      'Gastos y duración media de los viajes (monetario)', 'Tipo de visitante']\n",
        "\n",
        "    for file_name in FILES_TO_PROCESS:\n",
        "        print(f\"-> Procesando limpieza: {file_name}\")\n",
        "\n",
        "        try:\n",
        "            # PARCHE DEFINITIVO: Leer el archivo asumiendo la primera línea como cabecera (header=0).\n",
        "            # Esto corrige el error de \"Total\" en 13938.csv y funciona para el resto.\n",
        "            content = uploaded[file_name].decode('latin1')\n",
        "            df = pd.read_csv(io.StringIO(content), sep=';', decimal=',', header=0)\n",
        "\n",
        "            # Limpiar la cabecera: convertir a string y quitar espacios\n",
        "            df.columns = df.columns.astype(str).str.strip()\n",
        "\n",
        "            # --- 1. NORMALIZACIÓN DE COLUMNA DE VALOR ('TOTAL') ---\n",
        "            total_col_name = None\n",
        "            for col in df.columns:\n",
        "                # Buscamos 'Total' (que ya debería existir) o cualquier variación\n",
        "                if col.strip().lower() in ['total', 'valor', 'total nacional']:\n",
        "                    total_col_name = col\n",
        "                    break\n",
        "\n",
        "            if total_col_name is None:\n",
        "                # La columna 'Total' debería encontrarse aquí. Si no, es un error fatal.\n",
        "                raise KeyError(f\"No se pudo encontrar la columna de valor ('Total', 'VALOR', etc.) en {file_name}.\")\n",
        "\n",
        "            # Renombrar para estandarizar: si el nombre no es 'Total', lo renombramos\n",
        "            if total_col_name != 'Total':\n",
        "                df = df.rename(columns={total_col_name: 'Total'})\n",
        "\n",
        "            # --- 2. MANEJO DE DIMENSIONES (CASOS ESPECIALES) ---\n",
        "            df_clean = df.copy()\n",
        "\n",
        "            # Caso especial: Partidas de Gasto (13938.csv)\n",
        "            if file_name == '13938.csv':\n",
        "                df_clean['Clasificacion_Tipo'] = 'Partidas de Gasto'\n",
        "\n",
        "                levels = ['Partidas de gasto: Nivel 1', 'Partidas de gasto: Nivel 2', 'Partidas de gasto: Nivel 3']\n",
        "\n",
        "                # Para evitar errores de tipo en las columnas, las convertimos a string (si existen)\n",
        "                for level in levels:\n",
        "                    if level in df_clean.columns:\n",
        "                        df_clean[level] = df_clean[level].astype(str)\n",
        "\n",
        "                # Combinar los niveles en una sola columna Clasificacion_Valor\n",
        "                df_clean['Clasificacion_Valor'] = df_clean[levels].fillna('').agg(' - '.join, axis=1)\n",
        "\n",
        "                # Limpiar guiones redundantes\n",
        "                df_clean['Clasificacion_Valor'] = (\n",
        "                    df_clean['Clasificacion_Valor'].str.replace(r'\\s-\\s-\\s', ' - ', regex=True)\n",
        "                                                   .str.replace(r'^nan\\s-\\s', '', regex=True)\n",
        "                                                   .str.replace(r'^\\s*-\\s*', '', regex=True).str.strip()\n",
        "                )\n",
        "\n",
        "            # Caso General: Los otros 4 archivos (tienen una única columna de dimensión)\n",
        "            else:\n",
        "                dimension_cols = [col for col in df.columns if col not in EXCLUSION_COLS and col != 'Total']\n",
        "                dimension_col_name = dimension_cols[0] if dimension_cols else None\n",
        "\n",
        "                if dimension_col_name is None:\n",
        "                    print(f\"   Advertencia: No se encontró columna de dimensión principal en {file_name}. Saltando.\")\n",
        "                    continue\n",
        "\n",
        "                df_clean = df_clean.rename(columns={dimension_col_name: 'Clasificacion_Valor'})\n",
        "                df_clean['Clasificacion_Tipo'] = dimension_col_name\n",
        "\n",
        "            # --- 3. CREACIÓN DE INDICADOR DE GASTO ---\n",
        "            if 'Gastos y duración media de los viajes' in df_clean.columns:\n",
        "                df_clean = df_clean.rename(columns={'Gastos y duración media de los viajes': 'indicador_gasto'})\n",
        "            elif 'Tipo de dato' in df_clean.columns:\n",
        "                df_clean = df_clean.rename(columns={'Tipo de dato': 'indicador_gasto'})\n",
        "            else:\n",
        "                df_clean['indicador_gasto'] = 'Gasto no clasificado'\n",
        "\n",
        "            # --- 4. LIMPIEZA FINAL Y TIPOS ---\n",
        "            df_clean['Total'] = pd.to_numeric(df_clean['Total'], errors='coerce')\n",
        "            df_clean = df_clean.dropna(subset=['Total', 'Clasificacion_Valor'])\n",
        "\n",
        "            # --- 5. SELECCIÓN FINAL Y RENOMBRAMIENTO ---\n",
        "            FINAL_COLS = ['Clasificacion_Tipo', 'Clasificacion_Valor', 'indicador_gasto', 'Periodo', 'Total']\n",
        "            df_final = df_clean.reindex(columns=FINAL_COLS)\n",
        "\n",
        "            df_final.columns = ['clasificacion_tipo', 'clasificacion_valor', 'indicador_gasto', 'periodo', 'gasto_total']\n",
        "\n",
        "            all_data.append(df_final)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ ERROR CRÍTICO al procesar {file_name}. La causa fue: {e}\")\n",
        "            continue\n",
        "\n",
        "    # 6. UNIFICACIÓN y Post-Procesamiento (Creación de fecha)\n",
        "    if not all_data:\n",
        "        print(\"\\n❌ FALLO TOTAL: Ningún archivo pudo ser procesado con éxito.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_master = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    # Post-Procesamiento de la columna 'Periodo'\n",
        "    df_master['fecha_analisis'] = pd.to_datetime(\n",
        "        df_master['periodo'].astype(str).str.replace('M', ''),\n",
        "        format='%Y%m',\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "    df_master = df_master[[\n",
        "        'fecha_analisis',\n",
        "        'periodo',\n",
        "        'clasificacion_tipo',\n",
        "        'clasificacion_valor',\n",
        "        'indicador_gasto',\n",
        "        'gasto_total'\n",
        "    ]]\n",
        "\n",
        "    return df_master\n",
        "\n",
        "# Ejecutar la función de limpieza\n",
        "df_egatur_master = clean_and_unify_egatur()\n",
        "\n",
        "if not df_egatur_master.empty:\n",
        "    print(f\"\\n✅ ETL COMPLETO. Filas consolidadas en el DataFrame maestro: {len(df_egatur_master)}\")\n",
        "    print(\"\\nPrimeras 5 filas del DataFrame final (listo para PostgreSQL):\")\n",
        "    print(df_egatur_master.head())\n",
        "else:\n",
        "    print(\"\\n❌ ETL FALLIDA: El DataFrame maestro está vacío. Revise los errores anteriores.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hj53rQ9GvVW",
        "outputId": "efb13ca5-1902-407d-b9aa-037fe21c26ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Procesando limpieza: 13938.csv\n",
            "-> Procesando limpieza: 10838.csv\n",
            "-> Procesando limpieza: 10839.csv\n",
            "-> Procesando limpieza: 10828.csv\n",
            "-> Procesando limpieza: 23995.csv\n",
            "\n",
            "✅ ETL COMPLETO. Filas consolidadas en el DataFrame maestro: 10730\n",
            "\n",
            "Primeras 5 filas del DataFrame final (listo para PostgreSQL):\n",
            "  fecha_analisis  periodo clasificacion_tipo      clasificacion_valor  \\\n",
            "0     2025-09-01  2025M09  Partidas de Gasto  Gasto total - nan - nan   \n",
            "1     2025-08-01  2025M08  Partidas de Gasto  Gasto total - nan - nan   \n",
            "2     2025-07-01  2025M07  Partidas de Gasto  Gasto total - nan - nan   \n",
            "3     2025-06-01  2025M06  Partidas de Gasto  Gasto total - nan - nan   \n",
            "4     2025-05-01  2025M05  Partidas de Gasto  Gasto total - nan - nan   \n",
            "\n",
            "        indicador_gasto  gasto_total  \n",
            "0  Gasto no clasificado      105.828  \n",
            "1  Gasto no clasificado       92.463  \n",
            "2  Gasto no clasificado       76.074  \n",
            "3  Gasto no clasificado       59.622  \n",
            "4  Gasto no clasificado       46.586  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# 4. DESCARGA DEL ARCHIVO ETL MAESTRO (Backup del Progreso - CORREGIDO)\n",
        "# ----------------------------------------------------\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os # Importamos os para interactuar con el sistema de archivos\n",
        "\n",
        "def download_master_csv(df):\n",
        "    \"\"\"\n",
        "    Convierte el DataFrame maestro a CSV (separado por tabulación) y lo descarga.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"❌ ERROR: El DataFrame maestro está vacío. La Celda 3 no fue exitosa.\")\n",
        "        return\n",
        "\n",
        "    # Definir el nombre del archivo\n",
        "    file_name = f'egatur_datos_maestros_{pd.Timestamp.now().strftime(\"%Y%m%d\")}.csv'\n",
        "\n",
        "    # Escribir el DataFrame directamente en el sistema de archivos de Colab\n",
        "    # Usamos \\t como separador y \\N para NaNs, compatible con PostgreSQL/Airflow.\n",
        "    df.to_csv(\n",
        "        file_name, # Escribir al archivo en Colab\n",
        "        index=False,\n",
        "        sep='\\t',\n",
        "        na_rep='\\\\N',\n",
        "        encoding='utf-8',\n",
        "        columns=['fecha_analisis', 'periodo', 'clasificacion_tipo', 'clasificacion_valor', 'indicador_gasto', 'gasto_total']\n",
        "    )\n",
        "\n",
        "    # Forzar la descarga del archivo que acabamos de crear en el disco de Colab\n",
        "    files.download(file_name)\n",
        "\n",
        "    print(f\"\\n✅ ¡ÉXITO! El archivo '{file_name}' se ha descargado a tu máquina local.\")\n",
        "    print(\"Guarda este archivo. Es tu ETL limpio, listo para el análisis en Power BI o la carga en PostgreSQL.\")\n",
        "\n",
        "# Ejecutar la función de descarga\n",
        "download_master_csv(df_egatur_master)\n",
        "\n",
        "# Nota: Debes resolver el problema de PostgreSQL antes de continuar con la Celda 5 (Carga)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "7NjxM95CHudg",
        "outputId": "30ab064b-8dce-4891-ac10-d9a14512ce9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_edf573eb-2ced-4647-b826-e9dd5a8a28a8\", \"egatur_datos_maestros_20251106.csv\", 973138)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ ¡ÉXITO! El archivo 'egatur_datos_maestros_20251106.csv' se ha descargado a tu máquina local.\n",
            "Guarda este archivo. Es tu ETL limpio, listo para el análisis en Power BI o la carga en PostgreSQL.\n"
          ]
        }
      ]
    }
  ]
}